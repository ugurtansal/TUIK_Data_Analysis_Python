# -*- coding: utf-8 -*-
"""
Created on Thu Mar 13 13:21:24 2025

@author: ugurt
"""

#%%TanÄ±mlayÄ±cÄ± istatistikler

import pandas as pd

df=pd.read_excel(r'path')

df.info() #Kolonlar hakkÄ±nda tanÄ±mlayÄ±cÄ± bilgiler

df.Motor_hacmi.astype(int) #Ä°lgili deÄŸiÅŸken tipinin deÄŸiÅŸmesi

df['Motor_hacmi3']=df.Motor_hacmi.astype(int)



import numpy as np

df['Motor_hacmi2']=np.ceil(df['Motor_hacmi']).astype(int)


df.describe()

df.describe(include='all')

df.describe(include='all').T #Transpose u alÄ±nÄ±r

df_tanimlayici_ist=df.describe(include='all').T

df_tanimlayici_ist=df.describe(include='all').T.reset_index()







#%%SÄ±klÄ±k DaÄŸÄ±lÄ±mÄ±

df['Marka'].value_counts()


k=int(np.ceil(1+3.3*np.log(df.shape[0])))

df['SÄ±nÄ±f']=pd.cut(df['Sehir_ici'],bins=k) #SayÄ±sal bir deÄŸiÅŸken iÃ§in sÄ±nÄ±f aralÄ±klarÄ±nÄ±n belirlenmesi

df['SÄ±nÄ±f'].value_counts().sort_index() # SÄ±klÄ±k daÄŸÄ±lÄ±mÄ±nÄ±n sayÄ±sal bir deÄŸiÅŸken iÃ§in oluÅŸturulmasÄ± - SonuÃ§ta kÃ¶ÅŸeli parantez dahil olmamayÄ±, normal parantez dahilliÄŸi ifade eder.

#df["SÄ±nÄ±f1"]=  pd.cut(df['Sehir_ici'],bins = [9.95, 12.381, 14.762, 17.143, 19.524, 21.905, 24.286, 26.667, 29.048, 60.0])
#df['SÄ±nÄ±f1'].value_counts().sort_index()
#df["SÄ±nÄ±f3"]=  pd.qcut(df['Sehir_ici'],q=9)
#df['SÄ±nÄ±f3'].value_counts().sort_index()

# bu sonuÃ§lara bakÄ±ldÄ±ÄŸÄ±nda daÄŸÄ±lÄ±mÄ±n saÄŸa Ã§arpÄ±k olduÄŸu gÃ¶rÃ¼lmekte sÄ±nÄ±f sayÄ±sÄ±nÄ± deÄŸiÅŸtirerek tekrar oluÅŸturalÄ±m.

df['SÄ±nÄ±f2']=pd.cut(df['Sehir_ici'],bins=10) 

df['SÄ±nÄ±f2'].value_counts().sort_index()



#%% SÃ¼rekli verileri iÃ§in kullanÄ±labilecek gÃ¶sterimler (Grafik)

#Veri gÃ¶rselleÅŸtirme (gÃ¶rsellerin consola dÃ¼ÅŸmesi iÃ§in plots tabÄ±nda saÄŸ Ã¼stteki Ã¼Ã§ Ã§iÄŸiden ilk iki seÃ§enek iptal edilir.)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#Histogram

sns.histplot(df['Sehir_ici'], bins=9)  # KDE eÄŸrisi olmadan histogram

sns.histplot(df['Sehir_ici'], bins=9, kde=True)  # KDE eÄŸrisi olan histogram

#Boxplot
 # **ğŸ“Œ Y Ekseni SÄ±nÄ±rlarÄ±nÄ± Belirleme**
ymin, ymax = df['Sehir_ici'].min()*(0.8), df['Sehir_ici'].max()*(1.2)
sns.boxplot(y=df['Sehir_ici'])


fig,ax=plt.subplots()
sns.boxplot(y=df['Sehir_ici'], ax=ax,showmeans=True)
ax.set_title('BaÅŸlÄ±k')
plt.show()

"""
# Boxplot Ã§izme
fig, ax = plt.subplots(figsize=(6, 4))
sns.boxplot(y=df["Sehir_ici"], ax=ax)

# AykÄ±rÄ± Sehir_icii direkt plot Ã¼zerinden alma
outliers = df["Sehir_ici"][(df["Sehir_ici"] < df["Sehir_ici"].quantile(0.25) - 1.5 * (df["Sehir_ici"].quantile(0.75) - df["Sehir_ici"].quantile(0.25))) |
                          (df["Sehir_ici"] > df["Sehir_ici"].quantile(0.75) + 1.5 * (df["Sehir_ici"].quantile(0.75) - df["Sehir_ici"].quantile(0.25)))]

# AykÄ±rÄ± Sehir_icii kutu grafiÄŸi Ã¼zerine indeksleri ile ekleme
for i in outliers.index:
    ax.text(x=0, y=df["Sehir_ici"][i], s=str(i), color='red', ha='center', fontsize=10, fontweight='bold')

plt.title("Boxplot ve AykÄ±rÄ± GÃ¶zlem NumaralarÄ±")
plt.show()
"""

#Violin

fig,b=plt.subplots()
sns.violinplot(y=df['Sehir_ici'], ax=b, color="lightgreen")
b.set_title("BaÅŸlÄ±k")
plt.show()

#Violin ve box plotÄ±n birlikte gÃ¶sterimi

fig,b=plt.subplots()
vp = sns.violinplot(y=df['Sehir_ici'], ax=b, color="lightgreen", linewidth=1.5)
for artist in vp.collections:
    artist.set_alpha(0.4)  # %40 ÅŸeffaf yap

sns.boxplot(y=df['Sehir_ici'], ax=b, color="lightcoral", width=0.3)
b.set_title("Violin ve Box Plot Ãœst Ãœste")
plt.show()

#QQ Plot
import statsmodels.api as sm  # QQ plot iÃ§in lazÄ±m

fig,b=plt.subplots()
sm.qqplot(df['Sehir_ici'], line='s', ax=b)  # Normal daÄŸÄ±lÄ±m Ã§izgisi ile
b.set_title("QQ Plot")   
plt.show()

#%% Veri gÃ¶rselleÅŸtirme 
#(gÃ¶rsellerin consola dÃ¼ÅŸmesi iÃ§in plots tabÄ±nda saÄŸ Ã¼stteki Ã¼Ã§ Ã§iÄŸiden ilk iki seÃ§enek iptal edilir.)
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
#%% Kategorik veriler iÃ§in kullanÄ±labilecek gÃ¶sterimler

#Pie GrafiÄŸi
#Eski yÃ¶ntemde eklenecek

# Gruplayarak frekanslarÄ± otomatik hesapla ve pie chart Ã§iz
df.groupby('Orijin').size().plot.pie(autopct='%1.1f%%', figsize=(6,6))
plt.title("Kategori DaÄŸÄ±lÄ±mÄ±")
plt.ylabel("")  # Y-etiketini kaldÄ±r
plt.show()

""" pieÄ±n parÃ§alÄ± gÃ¶sterimi
df.groupby('Orijin').size().plot.pie(autopct='%1.1f%%', figsize=(6,6), startangle=90, explode=(0.1,0.1,0.1),shadow=True)
plt.title("Kategori DaÄŸÄ±lÄ±mÄ±")
plt.ylabel("")  # Y-etiketini kaldÄ±r
plt.show()
"""

# Kategori frekanslarÄ±nÄ± hesapla
kategori_sayilari = df.groupby('Orijin').size()

# Ã‡ubuk grafiÄŸi Ã§iz
kategori_sayilari.plot.bar(color=['red', 'blue', 'green'], figsize=(6, 4))

plt.title("Kategori DaÄŸÄ±lÄ±mÄ±")
plt.xlabel("Orijin")
plt.ylabel("Frekans")
plt.xticks(rotation=0)  # X eksenindeki yazÄ±larÄ± dÃ¼z hale getir
plt.grid(axis='y', alpha=0.7)  # Daha okunaklÄ± olmasÄ± iÃ§in yatay Ã§izgiler ekle
plt.show()

# Ã‡ubuk grafiÄŸi Ã§izme (Etiketler eklenerek)
kategori_sayilari = df.groupby('Orijin').size()
ax = kategori_sayilari.plot.bar(color=['red', 'blue', 'green'], figsize=(6, 4))

# Her Ã§ubuÄŸun Ã¼zerine sayÄ±larÄ± ekle (etiketleme)
for i, v in enumerate(kategori_sayilari.values):
    ax.text(i, 50, str(v), ha='center', fontsize=12, fontweight='bold', color='white')

# BaÅŸlÄ±k ve etiketler
plt.title("Kategori DaÄŸÄ±lÄ±mÄ±")
plt.xlabel("Kategoriler")
plt.ylabel("Frekans")
plt.xticks(rotation=0)  # X eksenindeki yazÄ±larÄ± dÃ¼z hale getir
plt.grid(axis='y', alpha=0.7)  # Daha okunaklÄ± olmasÄ± iÃ§in yatay Ã§izgiler ekle
plt.show()

# Scatter plot Ã§izimi

df.plot.scatter(x='Sehir_ici', y='Sehir_disi', title='Scatter Plot Ã–rneÄŸi')

#alternatif sns ile scatter plot Ã§izimi
sns.scatterplot(df['Sehir_ici'],df['Sehir_disi'])
plt.title("BaÅŸlÄ±k")

#%% UÃ§ AykÄ±rÄ± DeÄŸer Tespiti

Ort_Sehi=np.round(df['Sehir_ici'].mean(),2)
Std_Sehi=np.round(df['Sehir_ici'].std(),2)

up_b=Ort_Sehi+2*Std_Sehi
low_b=Ort_Sehi-2*Std_Sehi

df['Yeni']=(df['Sehir_ici']-Ort_Sehi)/Std_Sehi

df['Yeni2']=df['Yeni'].apply(lambda x: "anormal" if np.abs(x)>2 else "normal")

df['Yeni2'].value_counts()
df['Yeni2'].value_counts(normalize=True)

#Tekraralayan durum tespiti iÃ§in fonksiyon yazÄ±lmasÄ± uygundur

def anotest(af, c):
    af[c+'_yeni']=(af[c]-np.round(af[c].mean(),2))/np.round(af[c].std(),2)
    af[c+'_uyarÄ±']=af[c+'_yeni'].apply(lambda x: "anormal" if np.abs(x)>2 else "normal")
    return af

df=anotest(df,"Sehir_disi")

#UÃ§ aykÄ±rÄ± deÄŸer tespitinde medyan yÃ¶ntmei eklenecek.

#%% Normal DaÄŸÄ±lÄ±m Tespiti

import scipy.stats as stats

#Hipotez kurulmasÄ± - 1.Hipotez:Ä°stenilen deÄŸiÅŸken ile normal daÄŸÄ±lÄ±m arasÄ±nda fark yoktur Alternatif Hipotez:Ä°stenilen deÄŸiÅŸken ile normal daÄŸÄ±lÄ±m arasÄ±nda fark vardÄ±r.

ks_stat, ks_p = stats.kstest(df['Sehir_ici'],  "norm", args=(df['Sehir_ici'].mean(), df['Sehir_ici'].std()))

ks_stat, ks_p = stats.kstest(df['Uzunluk'],  "norm", args=(df['Uzunluk'].mean(), df['Uzunluk'].std()))

def ks_t(data,kolon,hata,dagilim="norm"):
    ks_stat, ks_p = stats.kstest(data[kolon], dagilim, args=(data[kolon].mean(), data[kolon].std()))
    ks_result = "DeÄŸiÅŸken daÄŸÄ±lÄ±mÄ± normal daÄŸÄ±lÄ±ma uymaktadÄ±r." if ks_p > hata else "DeÄŸiÅŸken daÄŸÄ±lÄ±mÄ± normal daÄŸÄ±lÄ±ma uymamaktadÄ±r."
    return ks_result

B=ks_t(df,"Uzunluk",0.05)
C=ks_t(df,"Sehir_ici",0.05)
#GÃ¶zlem sayÄ±sÄ±nÄ±n bÃ¼yÃ¼klÃ¼ÄŸÃ¼ne gÃ¶re normallik testi farklÄ±lÄ±k gÃ¶stermektedir.

#%% Normallik Fonksiyonu

def sw_t(data,kolon,hata):
    shapiro_stat, shapiro_p = stats.shapiro(data[kolon])
    shapiro_result = 0 if shapiro_p > hata else 1
    return shapiro_result

def ks_t(data,kolon,hata,dagilim="norm"):
    ks_stat, ks_p = stats.kstest(data[kolon], dagilim, args=(data[kolon].mean(), data[kolon].std()))
    ks_result = 0 if ks_p > hata else 1
    return ks_result


def ad_t(data,kolon,hata,dagilim="norm"):
    anderson_result = stats.anderson(data[kolon], dist=dagilim)
    anderson_stat = anderson_result.statistic
    
    # Anderson p-deÄŸeri hesaplama (kritik deÄŸerlere gÃ¶re tahmini p)
    if anderson_stat < anderson_result.critical_values[0]:
        anderson_p = 0.15
    elif anderson_stat < anderson_result.critical_values[1]:
        anderson_p = 0.10
    elif anderson_stat < anderson_result.critical_values[2]:
        anderson_p = 0.05
    elif anderson_stat < anderson_result.critical_values[3]:
        anderson_p = 0.025
    elif anderson_stat < anderson_result.critical_values[4]:
        anderson_p = 0.01
    else:
        anderson_p = 0.001  # Normal daÄŸÄ±lÄ±mdan en uzak deÄŸer
    
    ad_result = 0 if anderson_p > hata else 1
    return ad_result

    
def cvm_t(data,kolon,hata):
    cvm_result = stats.cramervonmises(data[kolon], 'norm', args=(data[kolon].mean(), data[kolon].std()))
    cvm_stat = cvm_result.statistic
    cvm_p = cvm_result.pvalue
    cvm_result_str = 0 if cvm_p > hata else 1
    return cvm_result_str


def jb_t(data,kolon,hata,grup=None):
    jb_stat, jb_p = stats.jarque_bera(data[kolon])
    jb_result = 0 if jb_p > hata else 1
    return jb_result 
    

def normallik(data,kolon,hata,grup=None):
    try:
        durum=[]
        data[kolon]
        
        #anderson darling (AD)
            #her veri setinde kullanÄ±labilir. Shaphiro wilk'den daha gÃ¼Ã§lÃ¼dÃ¼r.(kÃ¼Ã§Ã¼k verilerde daha gÃ¼Ã§lÃ¼dÃ¼r.) 
            #Ã‡ok bÃ¼yÃ¼k verilerde aÅŸÄ±rÄ± duyarlÄ±lÄ±k gÃ¶sterebilir. 
            #kenarlardaki sapmalarÄ± daha iyi yakalar.
        sonuc_ad_t=ad_t(data,kolon,hata,dagilim="norm")
        #print(f"test sonucu1: {sonuc_ad_t}")
        if sonuc_ad_t==0:
            if len(data)<50:
                y1="YanÄ±t GÃ¼Ã§lÃ¼"                
            else:
                y1="AÅŸÄ±rÄ± DuyarlÄ±lÄ±k olabilir"
            durum.append([kolon,"Anderson Darling",sonuc_ad_t,y1])
        else:
            if len(data)<50:
                y2="YanÄ±t GÃ¼Ã§lÃ¼, UÃ§ DeÄŸer kontrol edilmeli"               
            else:
                y2="AÅŸÄ±rÄ± DuyarlÄ±lÄ±k olabilir, UÃ§ DeÄŸer kontrol edilmeli"
            durum.append([kolon,"Anderson Darling",sonuc_ad_t,y2])
            
        #cramer-von mises (CVM)
            #n<5000 olduÄŸu durumlarda Ã¶nerilir. (AD) ye benzer ama merkezdeki sapmalara karÅŸÄ± daha duyarlÄ±dÄ±r.
            #kÃ¼Ã§Ã¼k veri setlerinde daha gÃ¼Ã§lÃ¼dÃ¼r.
            #Ã‡ok bÃ¼yÃ¼k verilerde aÅŸÄ±rÄ± duyarlÄ±lÄ±k gÃ¶sterebilir. 
        sonuc_cvm_t=cvm_t(data,kolon,hata)
        if sonuc_cvm_t==0:
            if len(data)<5000:
                y3="YanÄ±t GÃ¼Ã§lÃ¼"
            else:
                y3="AÅŸÄ±rÄ± DuyarlÄ±lÄ±k olabilir."
            durum.append([kolon,"CramÃ©r-von Mises",sonuc_cvm_t,y3])
        else:
            if len(data)<5000:
                y4="YanÄ±t GÃ¼Ã§lÃ¼, Merkezde Sapma olabilir."
                
            else:
                y4="AÅŸÄ±rÄ± DuyarlÄ±lÄ±k olabilir."
            durum.append([kolon,"CramÃ©r-von Mises",sonuc_cvm_t,y4])
            
            
        
        if len(data)<=50:
            #shaphiro wilk  (SW)
                #kÃ¼Ã§Ã¼k verilerde gÃ¼Ã§lÃ¼dÃ¼r.kÃ¼Ã§Ã¼k sapmalarÄ± bile yakalayabilir. BÃ¼yÃ¼k verilerde fazla duyarlÄ± olabilir
            
            sonuc_sw=sw_t(data,kolon,hata)
            if sonuc_sw==1:
                y5="YanÄ±t GÃ¼Ã§lÃ¼, veri setinde uÃ§ deÄŸer olaiblir"                
            else:
                y5="YanÄ±t GÃ¼Ã§lÃ¼,Normal DaÄŸÄ±lÄ±yor"
            durum.append([kolon,"Shaphiro Wilk",sonuc_sw,y5])
            
        
        else:
            #kolmogorov smirnov (KS)
                #normal daÄŸÄ±lÄ±m dÄ±ÅŸÄ±ndaki daÄŸÄ±lÄ±mlarÄ±nda test edilmesinde kullanÄ±lÄ±r.
                   #kÃ¼Ã§Ã¼k veri setlerinde duyarsÄ±zdÄ±r. UÃ§ deÄŸerlere dair duyarlÄ±lÄ±ÄŸÄ± azdÄ±r. KÃ¼Ã§Ã¼k veri setlerinde Ã¶nerilmez
            sonuc_ks=ks_t(data,kolon,hata,dagilim="norm")
            if sonuc_ks==0:
                y6="DaÄŸÄ±lÄ±m normal ancak uÃ§ deÄŸer olabilir."
            else:
                y6="Normal daÄŸÄ±lmÄ±yor. UÃ§ deÄŸer kontrolÃ¼ yapÄ±labilir"
            durum.append([kolon,"Kolmogrov Smirnov",sonuc_ks,y6])
                
            #Jarque-Bera (JB)
                #normal daÄŸÄ±lÄ±mÄ±n Ã§arpÄ±klÄ±ÄŸÄ± ve basÄ±klÄ±ÄŸÄ±nÄ± test eder.
                #kÃ¼Ã§Ã¼k veri setlerinde duyarsÄ±zdÄ±r. KÃ¼Ã§Ã¼k sapmalarÄ± kaÃ§Ä±rabilir.
                #bÃ¼yÃ¼k veri setlerinde Ã§arpÄ±klÄ±k ve basÄ±klÄ±ÄŸÄ± deÄŸerlendirmek aÃ§Ä±sÄ±ndan uygundur.
            sonuc_jb=jb_t(data,kolon,hata)
            if sonuc_jb==0:
                y7="Ã‡arpÄ±kÄ±k ve basÄ±klÄ±k yÃ¶nÃ¼nden sonuÃ§lar uyumlu"
            else:
                y7="Normal daÄŸÄ±lmamada Ã§arpÄ±klÄ±k veya basÄ±klÄ±k etkisi olabilir. AraÅŸtÄ±rÄ±lmalÄ±"
            durum.append([kolon,"Jarque Bera",sonuc_jb,y7])
            
            
        degerlendirme=pd.DataFrame(durum, columns=['kolon adÄ±','Test AdÄ±', 'DeÄŸer', 'AÃ§Ä±klama'])
        #print(degerlendirme["DeÄŸer"])
        normal_skor=degerlendirme["DeÄŸer"].mean()
        if degerlendirme["DeÄŸer"].mean()==1:
            ozet="DeÄŸiÅŸken normal daÄŸÄ±lmÄ±yor. UÃ§ aykÄ±rÄ± deÄŸer kontrol edin veya parametrik olmayan yÃ¶ntem deneyin."
        elif degerlendirme["DeÄŸer"].mean()==0.5:
            ozet="FarklÄ± testler iÃ§in farklÄ± sonuÃ§lar Ã§Ä±kÄ±yor. Detay tablosunu inceleyin"
        elif degerlendirme["DeÄŸer"].mean()==0:
            ozet="DaÄŸÄ±lÄ±m normal."
        elif degerlendirme["DeÄŸer"].mean()<0.5:
            ozet="Testler bÃ¼yÃ¼k oranda daÄŸÄ±lÄ±mÄ± normal gÃ¶stermekte. Detaylar inceleneilir"
        else:    
            ozet="***Testler bÃ¼yÃ¼k oranda daÄŸÄ±lÄ±mÄ± normal olmadÄ±ÄŸÄ±nÄ± gÃ¶stermekte. Detaylar inceleneilir"
        return ozet,normal_skor,degerlendirme
    except KeyError: #veri setinde yoksa bu hata dÃ¶ner
        print("ilgili kolon veri setinde bulunamadÄ± kontrol ediniz.")
    """
    except TypeError: #deÄŸiÅŸkenin tipi uygun deÄŸilse bu hata dÃ¶ner.
        print("Kolon tipi SÃ¼rekli deÄŸil kontrol ediniz")
    """    



#%% GÃ¼ven AralÄ±ÄŸÄ± HesaplanmasÄ±

from scipy.stats import t,norm


ort = df['Uzunluk'].mean()
std_sap = df['Uzunluk'].std()
n = df['Uzunluk'].count()  # Ã–rneklem bÃ¼yÃ¼klÃ¼ÄŸÃ¼
confidence_level = 0.95
alpha = 1 - confidence_level
degrees_of_freedom = n - 1  #
kritik=norm.ppf(1 - alpha / 2)

margin_of_error = kritik * (std_sap / np.sqrt(n))  # Hata payÄ±
confidence_interval = (ort - margin_of_error, ort + margin_of_error)

print(f"Uzunluk deÄŸiÅŸkeninin %95 lik gÃ¼ven aralÄ±ÄŸÄ± {confidence_interval} dÄ±r")



def guv_aralik(veri,kolon,guven):
    from scipy.stats import t,norm
    import matplotlib.pyplot as plt
    import numpy as np
    ort = veri[kolon].mean()
    std_sap = veri[kolon].std()
    n = veri[kolon].count()  # Ã–rneklem bÃ¼yÃ¼klÃ¼ÄŸÃ¼

    # %95 gÃ¼ven aralÄ±ÄŸÄ± iÃ§in t-daÄŸÄ±lÄ±mÄ±ndan kritik deÄŸeri al
    confidence_level = guven
    alpha = 1 - confidence_level
    degrees_of_freedom = n - 1  # Serbestlik derecesi
    #t_critical = t.ppf(1 - alpha / 2, degrees_of_freedom)  # t-daÄŸÄ±lÄ±mÄ±ndan kritik deÄŸer. gÃ¶zlem deÄŸeri 30'dan kÃ¼Ã§Ã¼k ise
    #z_critical = norm.ppf(1 - alpha / 2)  # z-daÄŸÄ±lÄ±mÄ±ndan kritik deÄŸer. gÃ¶zlem deÄŸeri 30'dan bÃ¼yÃ¼k ise

    ##gÃ¶zlem sayÄ±sÄ±na gÃ¶re kritik deÄŸeri belirle
    if n<30:
        kritik=t.ppf(1 - alpha / 2, degrees_of_freedom)  # t-daÄŸÄ±lÄ±mÄ±ndan kritik deÄŸer. gÃ¶zlem deÄŸeri 30'dan kÃ¼Ã§Ã¼k ise
        uyarÄ±="not: gÃ¶zlem sayÄ±sÄ± 30'dan kÃ¼Ã§Ã¼k olduÄŸu iÃ§in t tablosu kullanÄ±lmÄ±ÅŸtÄ±r."
    else:
        kritik=norm.ppf(1 - alpha / 2)  # z-daÄŸÄ±lÄ±mÄ±ndan kritik deÄŸer. gÃ¶zlem deÄŸeri 30'dan bÃ¼yÃ¼k ise
        uyarÄ±="not: gÃ¶zlem sayÄ±sÄ± 30'dan bÃ¼yÃ¼k olduÄŸu iÃ§in z tablosu kullanÄ±lmÄ±ÅŸtÄ±r."
    # GÃ¼ven aralÄ±ÄŸÄ±nÄ± hesapla
    margin_of_error = kritik * (std_sap / np.sqrt(n))  # Hata payÄ±
    confidence_interval = (ort - margin_of_error, ort + margin_of_error)
    # SonuÃ§larÄ± yazdÄ±r
    
    print(baslik := f"{kolon} deÄŸiÅŸkeni iÃ§in %95 GÃ¼ven AralÄ±ÄŸÄ±:")
    print(ifade2 :=f"  Ortalama: {ort:.2f}")
    print(ifade3 :=f"  GÃ¼ven AralÄ±ÄŸÄ±: ({confidence_interval[0]:.2f}, {confidence_interval[1]:.2f})")
    print(uyarÄ±)
    
    # Grafiksel gÃ¶sterim
    plt.figure(figsize=(8, 4))
    plt.errorbar(x=kolon, y=ort, yerr=margin_of_error, fmt='o', color='blue', 
                 capsize=5, label='Ortalama ve GÃ¼ven AralÄ±ÄŸÄ±')
    plt.title(f"{kolon} iÃ§in %95 GÃ¼ven AralÄ±ÄŸÄ±")
    plt.xlabel("DeÄŸiÅŸken")
    plt.ylabel("DeÄŸer")
    
    
    # Y ekseni sÄ±nÄ±rlarÄ±nÄ± dinamik olarak ayarla
    plt.ylim(ort - 2 * margin_of_error, ort + 2 * margin_of_error)
    
    # Bilimsel gÃ¶sterimi kapat
    plt.ticklabel_format(style='plain', axis='y')
    plt.legend()
    plt.grid(True)
    plt.show()
    sonuc=baslik+"\n"+ifade2+"\n"+ifade3+"\n"+uyarÄ±
    return sonuc

#%% Tek Ã¶rneklem testi

#Hipotez kurulmasÄ± - 1.Hipotez:Ä°stenilen deÄŸiÅŸken iÃ§in test deÄŸerinin kitle ortalamasÄ± arasÄ±nda fark yoktur 


print("GÃ¶zlem SayÄ±sÄ± 30'dan bÃ¼yÃ¼k olduÄŸu iÃ§in Z istatistiÄŸi kullanÄ±lacaktÄ±r")
n = len(df['Uzunluk'].dropna())
x_bar = np.mean(df['Uzunluk'].dropna())  # Ã–rneklem ortalamasÄ±
s = np.std(df['Uzunluk'].dropna(), ddof=1)  # Ã–rneklem standart sapmasÄ± (n-1 ile dÃ¼zeltilmiÅŸ)
s2 = np.std(df['Uzunluk'].dropna())  # Ã–rneklem standart sapmasÄ± (n-1 ile dÃ¼zeltilmiÅŸ)
test=185
# Z-istatistiÄŸi hesaplama (Ã¶rneklem sapmasÄ±nÄ± kullanarak)
z_stat = (x_bar - test) / (s / np.sqrt(n))

# P-deÄŸerini hesapla (Ã§ift yÃ¶nlÃ¼ test iÃ§in)
p_value = 2 * (1 - norm.cdf(abs(z_stat)))

if p_value>0.05:
    sonuc="FarkÄ± yoktur, Yokluk hipotezi kabul" 
else:
    sonuc="FarkÄ± vardÄ±r, Alternatif hipotez kabul"
    
print(f"Uzunluk deÄŸiÅŸkeninin %95 gÃ¼ven dÃ¼zeyinde {test} kitle ortalamasÄ±ndan {sonuc}.")

#EÄŸer veri seti normal daÄŸÄ±lmÄ±yorsa non parametrik testlerden wilcoxon testi kullanÄ±lÄ±r.

test_stat, p_value = stats.wilcoxon(df['Uzunluk'].dropna() - test)
#print(f"Wilcoxon Test Ä°statistiÄŸi: {test_stat:.4f}")
print(f"P-DeÄŸeri: {p_value:.2f}")
if p_value < alpha:
    print(f"SonuÃ§: Medyan {test}'ten anlamlÄ± derecede farklÄ±dÄ±r (H0 reddedildi).")
else:
    print(f"SonuÃ§: Medyan {test} yakÄ±n olup anlamlÄ± fark yoktur (H0 kabul edildi).")

#%% BaÄŸÄ±msÄ±z iki Ã¶rneklem testi

#Hipotez kurulmasÄ± - 1.Hipotez:Asya ve Avrupa'da Ã¼retilen araÃ§larÄ±n uzunluklarÄ± arasÄ±nda %95 gÃ¼ven dÃ¼zeyinde fark bulunmamaktadÄ±r.

grup1=df[["Orijin","Uzunluk"]][df["Orijin"] == "Asya"]
grup2=df[["Orijin","Uzunluk"]][df["Orijin"] == "Avrupa"]
normal_k=0
varyans_k=0
   
for k in [["Asya",grup1],["Avrupa",grup2]]:
    
    print(f"{k[0]} grubundaki gÃ¶zlem sayÄ±sÄ± {len(k[1]['Uzunluk'])}, ortalama deÄŸer {k[1]['Uzunluk'].mean():.2f}")
    #print(f"{k[1]},{test_kolon},{alpha}")
    sonuc=ks_t(k[1],"Uzunluk",0.05)
    if sonuc=="DeÄŸiÅŸken daÄŸÄ±lÄ±mÄ± normal daÄŸÄ±lÄ±ma uymaktadÄ±r.":
        print(f"{k[0]} verileri normal daÄŸÄ±lmaktadÄ±r")
    else:
        print(f"**{k[0]} verileri normal daÄŸÄ±lmamaktadÄ±r")
        normal_k=1
    print("-"*40)
    
    
if normal_k==0:
    print("tÃ¼m deÄŸiÅŸkenler normal daÄŸÄ±lmaktadÄ±r.")
else:
    print("**normal daÄŸÄ±lma varsayÄ±mÄ± saÄŸlanmamÄ±ÅŸtÄ±r.")
print("-"*40)
    
print("EÅŸit VaryanslÄ±lÄ±k iÃ§in Levene Testi uygulanacaktÄ±r")
levene_stat, levene_p = stats.levene(grup1['Uzunluk'], grup2['Uzunluk'])
# p deÄŸeri 0.05'ten bÃ¼yÃ¼kse varyanslar eÅŸittir, deÄŸilse eÅŸit deÄŸildir
equal_var = True if levene_p > alpha else False
if equal_var:
    print("Ä°ki grubun varyans daÄŸÄ±lÄ±mÄ± benzerdir.")
else:
    print("**Ä°ki grubun varyans daÄŸÄ±lÄ±mÄ± farklÄ±dÄ±r.")
    varyans_k=1
print("-"*40)    
if normal_k==0:
    print("Veriler normal daÄŸÄ±ldÄ±ÄŸÄ± iÃ§in parametrik test uygulanacaktÄ±r.")
    # BaÄŸÄ±msÄ±z Ä°ki Ã–rneklem T-Testi
    t_stat, p_value = stats.ttest_ind(grup1['Uzunluk'], grup2['Uzunluk'], equal_var=equal_var)
    print(f"T-istatistiÄŸi: {t_stat.item():.4f}, p-deÄŸeri: {p_value.item():.4f}")
    
    
else:
    print("***Normallik varsayÄ±mÄ± saÄŸlanmamÄ±ÅŸtÄ±r. Parametrik olmayan test uygulanacaktÄ±r.")
    print("Man whitney U testi yapÄ±lacaktÄ±r.")
    stat, p_value = stats.mannwhitneyu(grup1['Uzunluk'], grup2['Uzunluk'], alternative='two-sided')
    print(f"U-istatistiÄŸi: {stat.item():.4f}, p-deÄŸeri: {p_value.item():.4f}")
    
if p_value<alpha:
    print("***SonuÃ§: H0 reddedildi, gruplar arasÄ±nda anlamlÄ± bir fark vardÄ±r.")
else:
    print("SonuÃ§: H0 kabul edildi, gruplar arasÄ±nda anlamlÄ± bir fark bulunamamÄ±ÅŸtÄ±r.")
print("-"*40)
   
D=guv_aralik(grup1,"Uzunluk",0.95)
E=guv_aralik(grup2,"Uzunluk",0.95)

#%% BaÄŸÄ±mlÄ± iki grup testi

#Hipotez kurulmasÄ± - 1.Hipotez:Veri setindeki araÃ§larÄ±n ÅŸehir iÃ§i yakÄ±t tÃ¼ketimleri ile ÅŸehir dÄ±ÅŸÄ± yakÄ±t tÃ¼ketimleri arasÄ±nda fark yoktur.


df["Fark"]=df['Sehir_ici']-df['Sehir_disi']
sonuc=ks_t(df,"Fark",0.05)

if sonuc=="DeÄŸiÅŸken daÄŸÄ±lÄ±mÄ± normal daÄŸÄ±lÄ±ma uymaktadÄ±r.":
    print("Fark verileri normal daÄŸÄ±lmaktadÄ±r.\nParametrik test kullanÄ±lacaktÄ±r")
    t_stat, p_value = stats.ttest_rel(df['Sehir_ici'], df['Sehir_disi'])
    print(f"EÅŸleÅŸtirilmiÅŸ t-Testi Sonucu: T-istatistiÄŸi = {t_stat:.4f}, p-deÄŸeri = {p_value:.4f}")

else:
    print("**Fark verileri normal daÄŸÄ±lmamaktadÄ±r.\nParametrik olmayan Wilcoxon iÅŸaret testi kullanÄ±lacaktÄ±r")
    wilcoxon_stat, p_value = stats.wilcoxon(df['Sehir_ici'], df['Sehir_disi'])
    
    print(f"Wilcoxon Test Sonucu: Test istatistiÄŸi = {wilcoxon_stat:.4f}, p-deÄŸeri = {p_value:.4f}")
    
if p_value < alpha:
    print("SonuÃ§: KarÅŸÄ±laÅŸtÄ±rÄ±lan deÄŸiÅŸkenler arasÄ±nda anlamlÄ± bir fark vardÄ±r. (H0 reddedildi).")
else:
    print("SonuÃ§: KarÅŸÄ±laÅŸtÄ±rÄ±lan deÄŸiÅŸkenler anlamlÄ± bir fark bulunamamÄ±ÅŸtÄ±r. (H0 kabul edildi).")


#%% Varyans Analizi

"""def varyans_analiz(veri,grup_kolon,test_kolon,alpha):
    normal_k=0
    varyans_k=0
    post_h=0
    for grup in veri[grup_kolon].unique():
        secim=veri[veri[grup_kolon] == grup][[test_kolon,grup_kolon]]
        print(f"{grup} grubundaki gÃ¶zlem sayÄ±sÄ± {len(secim)}, ortalama deÄŸer {secim[test_kolon].mean():.2f}")
        sonuc=normallik(secim,test_kolon,alpha)
        if sonuc[1]<=0.5:
            print(f"{grup} verileri normal daÄŸÄ±lmaktadÄ±r")
        else:
            print(f"**{grup} verileri normal daÄŸÄ±lmamaktadÄ±r")
            normal_k=1
        print("-"*40)
    if normal_k==0:
        print("TÃ¼m altgruplar iÃ§in normal daÄŸÄ±lma koÅŸulu saÄŸlanmaktadÄ±r.")
    else:
        print("***Normallik varsayÄ±mÄ± saÄŸlanmamÄ±ÅŸtÄ±r.")
    print("-"*40)
    
    ##gruplar varyans homojenliÄŸi testi
    grup_listesi = [veri[veri[grup_kolon] == grup2][test_kolon].values for grup2 in veri[grup_kolon].unique()]
    stat, p = stats.levene(*grup_listesi)
    print(f"Levene Testi: p-deÄŸeri = {p:.4f}")
    equal_var = True if p > alpha else False
    if equal_var:
        print("GruplarÄ±n varyanslarÄ± homojendir.")
    else:
        print("***Gruplardan az 1 tanesinin varyansÄ± diÄŸerlerinden farklÄ±dÄ±r. ")
        varyans_k=1
    print("-"*40)
    
    
    # Welch ANOVA modeli oluÅŸturma
    model = ols(f"{test_kolon} ~ {grup_kolon}", data=veri).fit()
    welch_anova = sm.stats.anova_lm(model, typ=2, robust="hc3")  # HC3 Welch dÃ¼zeltmesi

    # SonuÃ§larÄ± yazdÄ±r
    print("Welch ANOVA sonucu:\n", welch_anova)
    
    if normal_k==0 and varyans_k==0:
        print("Hem Normallik hem de eÅŸit varyanslÄ±lÄ±k saÄŸlanmaktadÄ±r. Parametrik ANOVA testi yapÄ±lacaktÄ±r.")
        anova_stat, p_anv_value = stats.f_oneway(*grup_listesi)
        print(f"ANOVA Testi Sonucu: F-istatistiÄŸi = {anova_stat:.4f}, p-deÄŸeri = {p_anv_value:.4f}")
        
        if p_anv_value>alpha:
            print("GruplarÄ±n varyans daÄŸÄ±lÄ±mÄ± benzerdir.")
        else:
            print("Gruplardan en az 1 tanesi diÄŸerlerinden farklÄ± ortalamaya sahiptir")
            print("post-hoc test yapÄ±lmalÄ±dÄ±r (parametrik)")
            post_h=1
        
    elif normal_k==0 and varyans_k==1:
        print("Normallik SaÄŸlanmakta ancak EÅŸit varyanslÄ±lÄ±k saÄŸlanmamaktadÄ±r. Welch ANOVA testi yapÄ±laca**ktÄ±r.")
    elif normal_k==1 and varyans_k==0:
        print("***Normallik SaÄŸlanmamakta ancak EÅŸit varyanslÄ±lÄ±k saÄŸlanmaktadÄ±r. Kruskal Wallis testi yapÄ±lacaktÄ±r.")
    else:
        print("***Normallik ve EÅŸit varyanslÄ±lÄ±k saÄŸlanmamaktadÄ±r. Kruskal Wallis testi yapÄ±lacaktÄ±r.")
    print("-"*40)
#Fonksiyon eksik - """

# Kruskal-Wallis Testi

grup1=df["Uzunluk"][df["Orijin"] == "Asya"]
grup2=df["Uzunluk"][df["Orijin"] == "Avrupa"]
grup3=df["Uzunluk"][df["Orijin"] == "Amerika"]

h_stat, p_value_kw = stats.kruskal(grup1, grup2, grup3)

print("\n=== Kruskal-Wallis Testi ===")
print(f"H istatistiÄŸi: {h_stat}")
print(f"P-deÄŸeri: {p_value_kw}")

if p_value_kw < 0.05:
    print("Gruplar arasÄ±nda en az bir fark var (H0 reddedildi).")
else:
    print("Gruplar arasÄ±nda fark yok (H0 kabul edildi).")

# Kruskal sonrasÄ± Dunn-Bonferroni
import scikit_posthocs as sp

df_dunn = sp.posthoc_dunn([grup1, grup2, grup3], p_adjust='bonferroni')
print(np.round(df_dunn,8))

#Parametrik post-doc testi

from statsmodels.stats.multicomp import pairwise_tukeyhsd
# Veriyi uzun formata getir
"""data = np.concatenate([grup1, grup2, grup3])
groups = (['Group 1'] * len(grup1)) + (['Group 2'] * len(grup2)) + (['Group 3'] * len(grup3))

# DataFrame oluÅŸtur
dfd = pd.DataFrame({"Values": data, "Groups": groups})

# Tukey's HSD testi
tukey_results = pairwise_tukeyhsd(dfd['Values'], dfd['Groups'], alpha=0.05)"""
tukey_results2 = pairwise_tukeyhsd(df['Uzunluk'], df['Orijin'], alpha=0.05)

print(tukey_results2)

#%% Korelasyon

df.select_dtypes(include=[np.number]).corr(method="pearson")

from scipy.stats import pearsonr
df_numeric = df.select_dtypes(include=[np.number])
p_values = pd.DataFrame(index=df_numeric.columns, columns=df_numeric.columns)
r_values = pd.DataFrame(index=df_numeric.columns, columns=df_numeric.columns)
for col1 in df_numeric.columns:
    for col2 in df_numeric.columns:
        if col1 != col2:  # AynÄ± sÃ¼tunlarÄ±n korelasyonuna gerek yok
            r, p = pearsonr(df_numeric[col1], df_numeric[col2])
            p_values.loc[col1, col2] = p
            r_values.loc[col1, col2] = r
        else:
            p_values.loc[col1, col2] = np.nan
            r_values.loc[col1, col2] = np.nan


# TÃ¼m deÄŸiÅŸken Ã§iftleri iÃ§in Pearson korelasyonu ve p-deÄŸerlerini hesaplama
results=[]
columns = df_numeric.columns
for i in range(len(columns)):
    for j in range(i + 1, len(columns)):  # AynÄ± Ã§iftleri tekrar hesaplamamak iÃ§in
        col1, col2 = columns[i], columns[j]
        r, p = pearsonr(df_numeric[col1], df_numeric[col2])
        results.append([f"{col1} - {col2}", round(r, 2), round(p, 3)])

# SonuÃ§larÄ± DataFrame'e Ã§evir
results_df = pd.DataFrame(results, columns=["DeÄŸiÅŸken Ã‡ifti", "Pearson r", "P-DeÄŸeri"])

r_values

#%% Regresyon

import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import shapiro
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.stats.diagnostic import het_breuschpagan
from statsmodels.stats.stattools import durbin_watson

y = df["Beygir_gucu"]
X = df[["Motor_hacmi"]]

# ğŸ”¹ Sabit terimi ekleme
X = sm.add_constant(X)

# ğŸ”¹ Regresyon Modeli
model = sm.OLS(y, X).fit()

# ğŸ”¹ Model Ã–zetini YazdÄ±rma
print(model.summary())

### ğŸ”¥ VarsayÄ±m Testleri

## 1ï¸âƒ£ Ã‡oklu DoÄŸrusal BaÄŸÄ±ntÄ± (Multicollinearity) - VIF
vif_data = pd.DataFrame()
vif_data["DeÄŸiÅŸken"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
print("\nğŸ“Œ VIF DeÄŸerleri (Ã‡oklu DoÄŸrusal BaÄŸÄ±ntÄ±):\n", vif_data) #VIF deÄŸerleri const harici 10 dan kÃ¼Ã§Ã¼k olduÄŸunda Ã§oklu baÄŸlantÄ± olmadÄ±ÄŸÄ±nÄ± gÃ¶sterir.

## 2ï¸âƒ£ Hata Terimlerinin NormalliÄŸi - Shapiro-Wilk Testi
residuals = model.resid
shapiro_test = shapiro(residuals)
print(f"\nğŸ“Œ Shapiro-Wilk Testi Sonucu: Test Ä°statistiÄŸi={shapiro_test.statistic}, p-deÄŸeri={shapiro_test.pvalue}")

## 3ï¸âƒ£ Hata Terimlerinin DeÄŸiÅŸen VaryanslÄ±lÄ±ÄŸÄ± (Homoskedasticity) - Breusch-Pagan Testi
bp_test = het_breuschpagan(residuals, X)
print(f"\nğŸ“Œ Breusch-Pagan Testi Sonucu: LM Stat={bp_test[0]}, p-deÄŸeri={bp_test[1]}")

## 4ï¸âƒ£ Otokorelasyon (BaÄŸÄ±mlÄ±lÄ±k) - Durbin-Watson Testi
dw_stat = durbin_watson(residuals)
print(f"\nğŸ“Œ Durbin-Watson Testi (Otokorelasyon): {dw_stat}")

### ğŸ“Š GÃ¶rselleÅŸtirmeler

# ğŸ”¹ ArtÄ±klarÄ±n DaÄŸÄ±lÄ±mÄ± (Normallik KontrolÃ¼)
plt.figure(figsize=(12, 5))
sns.histplot(residuals, kde=True, bins=20)
plt.title("Hata Terimlerinin HistogramÄ±")
plt.xlabel("Hata Terimleri")
plt.show()

# ğŸ”¹ Q-Q Plot (NormalliÄŸi Kontrol Etmek Ä°Ã§in)
sm.qqplot(residuals, line="45", fit=True)
plt.title("Q-Q Plot (Normallik KontrolÃ¼)")
plt.show()

# ğŸ”¹ ArtÄ±klarÄ±n Serpme GrafiÄŸi (Homoskedastisite KontrolÃ¼)
plt.figure(figsize=(10, 5))
plt.scatter(model.fittedvalues, residuals, alpha=0.6)
plt.axhline(0, linestyle='dashed', color='red')
plt.title("ArtÄ±klarÄ±n DaÄŸÄ±lÄ±mÄ±")
plt.xlabel("Tahmin Edilen DeÄŸerler")
plt.ylabel("ArtÄ±klar")
plt.show()

# ğŸ”¹ Tahmin ile GerÃ§ekleÅŸen Ä°liÅŸki GrafiÄŸi
plt.figure(figsize=(10, 5))
plt.scatter(model.fittedvalues, df["Beygir_gucu"], alpha=0.6)
plt.title("Tahmin ile GerÃ§ekleÅŸen Ä°liÅŸki GrafiÄŸi")
plt.xlabel("Tahmin Edilen DeÄŸerler")
plt.ylabel("GerÃ§ek DeÄŸer")
plt.show()

#%% RxC TablolarÄ±

import pandas as pd
import numpy as np
from scipy.stats import chi2_contingency, fisher_exact

cross_tab = pd.crosstab(df["Orijin"], df["Tip"])

# ğŸ”¹ Ki-Kare Testi
chi2, p, dof, expected = chi2_contingency(cross_tab)

expected.round(2)
# ğŸ“Œ VarsayÄ±m 1: BaÄŸÄ±msÄ±zlÄ±k VarsayÄ±mÄ± (Bireylerin birden fazla kez sayÄ±lmadÄ±ÄŸÄ±nÄ± kontrol et)
# (Bunu test etmek iÃ§in genellikle veri toplama yÃ¶ntemine bakÄ±lÄ±r, kod ile doÄŸrudan test edilemez.)

# ğŸ“Œ VarsayÄ±m 2: Beklenen Frekanslar 5â€™ten BÃ¼yÃ¼k mÃ¼?
expected_df = pd.DataFrame(expected, index=cross_tab.index, columns=cross_tab.columns)
print("\nğŸ“Š Beklenen Frekanslar:")
print(expected_df)

# ğŸ”¹ Minimum beklenen frekansÄ± kontrol et
min_expected = np.min(expected)
if min_expected < 5:
    print("\nâš ï¸ En kÃ¼Ã§Ã¼k beklenen frekans deÄŸeri:", min_expected)
    print("âŒ Ki-Kare testi uygun olmayabilir. Fisher's Exact Test kullanÄ±labilir.")

# ğŸ”¹ ArtÄ±k Hesaplama
observed = cross_tab.values
expected = np.array(expected)

# StandartlaÅŸtÄ±rÄ±lmÄ±ÅŸ ArtÄ±klar (Standardized Residuals)
std_residuals = (observed - expected) / np.sqrt(expected)

# ğŸ”¹ ArtÄ±klarÄ± DataFrame olarak gÃ¶ster
std_residuals_df = pd.DataFrame(std_residuals, index=cross_tab.index, columns=cross_tab.columns)

print("\nğŸ“Œ StandartlaÅŸtÄ±rÄ±lmÄ±ÅŸ ArtÄ±klar (Standardized Residuals):")
print(std_residuals_df)

# ğŸ”¹ AnlamlÄ± ArtÄ±klarÄ± Belirleme (Z-test'e GÃ¶re)
significant_residuals = np.abs(std_residuals) > 2  # |SR| > 2 olan hÃ¼creler anlamlÄ± farklÄ±dÄ±r

print("\nğŸ“Œ AnlamlÄ± ArtÄ±klar (|SR| > 2 olan hÃ¼creler):")
print(pd.DataFrame(significant_residuals, index=cross_tab.index, columns=cross_tab.columns))
# ğŸ”¹ Yorumlama:
print("\nğŸ“Œ Yorum:")
for i in range(std_residuals.shape[0]):
    for j in range(std_residuals.shape[1]):
        if np.abs(std_residuals[i, j]) > 2:
            print(f"ğŸš¨ '{cross_tab.index[i]}' & '{cross_tab.columns[j]}' hÃ¼cresinde beklenenden Ã–NEMLÄ° DERECEDE farklÄ± bir iliÅŸki var! (SR = {std_residuals[i, j]:.2f})")




#%% Notlar

def bagimli_ornek(veri,kolon1,kolon2,alpha):
    veri["Fark"]=veri[kolon1]-veri[kolon2]
    ##fark verilerin normal daÄŸÄ±lÄ±p daÄŸÄ±lmadÄ±ÄŸÄ± kontrol edilir.
    print(f'Fark verilerindeki gÃ¶zlem sayÄ±sÄ± {len(veri["Fark"])}, ortalama fark {veri["Fark"].mean() :.2f}')
    sonuc=normallik(veri,"Fark",alpha)
    #print(sonuc[0],"---",sonuc[1])
    if sonuc[1]<=0.5:
        print("Fark verileri normal daÄŸÄ±lmaktadÄ±r.\nParametrik test kullanÄ±lacaktÄ±r")
        t_stat, p_value = stats.ttest_rel(veri[kolon1], veri[kolon2])
        print(f"EÅŸleÅŸtirilmiÅŸ t-Testi Sonucu: T-istatistiÄŸi = {t_stat:.4f}, p-deÄŸeri = {p_value:.4f}")

    else:
        print("**Fark verileri normal daÄŸÄ±lmamaktadÄ±r.\nParametrik olmayan Wilcoxon iÅŸaret testi kullanÄ±lacaktÄ±r")
        wilcoxon_stat, p_value = stats.wilcoxon(veri[kolon1], veri[kolon2])
        
        print(f"Wilcoxon Test Sonucu: Test istatistiÄŸi = {wilcoxon_stat:.4f}, p-deÄŸeri = {p_value:.4f}")
        
    if p_value < alpha:
        print("SonuÃ§: KarÅŸÄ±laÅŸtÄ±rÄ±lan deÄŸiÅŸkenler arasÄ±nda anlamlÄ± bir fark vardÄ±r. (H0 reddedildi).")
    else:
        print("SonuÃ§: KarÅŸÄ±laÅŸtÄ±rÄ±lan deÄŸiÅŸkenler anlamlÄ± bir fark bulunamamÄ±ÅŸtÄ±r. (H0 kabul edildi).")
    
    return sonuc




def tek_ornek(veri,kolon,test,alpha):
    #ilk olarak veri bÃ¼yÃ¼klÃ¼ÄŸÃ¼ kontrol edilir
    from scipy.stats import ttest_1samp
    from scipy.stats import norm
    
    ##ilk olarak verinin normalliÄŸi kontrol edilir.
    print(f"Veri setindeki gÃ¶zlem sayÄ±sÄ± {len(veri[kolon])}, ortalama deÄŸer {round(veri[kolon].mean(),2)}")
    sonuc=normallik(veri,kolon,alpha)
    print(f"{sonuc[1]}")
    if sonuc[1]<=0.5:
        print("deÄŸiÅŸken normal daÄŸÄ±lmaktadÄ±r")
        if len(veri[kolon].dropna())>30:
            print("GÃ¶zlem SayÄ±sÄ± 30'dan bÃ¼yÃ¼k olduÄŸu iÃ§in Z istatistiÄŸi kullanÄ±lacaktÄ±r")
            n = len(veri[kolon].dropna())
            x_bar = np.mean(veri[kolon].dropna())  # Ã–rneklem ortalamasÄ±
            s = np.std(veri[kolon].dropna(), ddof=1)  # Ã–rneklem standart sapmasÄ± (n-1 ile dÃ¼zeltilmiÅŸ)
    
            # Z-istatistiÄŸi hesaplama (Ã¶rneklem sapmasÄ±nÄ± kullanarak)
            z_stat = (x_bar - test) / (s / np.sqrt(n))
    
            # P-deÄŸerini hesapla (Ã§ift yÃ¶nlÃ¼ test iÃ§in)
            p_value = 2 * (1 - norm.cdf(abs(z_stat)))
        else:
            print("GÃ¶zlem SayÄ±sÄ± 30'dan kÃ¼Ã§Ã¼k olduÄŸu iÃ§in t istatistiÄŸi kullanÄ±lacaktÄ±r")
            t_statistic, p_value = ttest_1samp(veri[kolon].dropna(), test)  # NaN deÄŸerleri kaldÄ±r
        
        if p_value < alpha:
            print(f"**SonuÃ§: H0 hipotezi reddedildi, Ã¶rneklem ortalamasÄ± {test} deÄŸerinden farklÄ±dÄ±r.")
        else:
            print(f"SonuÃ§: H0 hipotezi reddedilemedi, Ã¶rneklem ortalamasÄ± {test} deÄŸerinden farksÄ±zdÄ±r.")
       
    else:
        print("**deÄŸiÅŸken normal daÄŸÄ±lmamaktadÄ±r")
        print("veri setinde non parametrik test uygulanmalÄ±dÄ±r")
        # Wilcoxon Ä°ÅŸaretli SÄ±ralar Testi
        test_stat, p_value = stats.wilcoxon(veri[kolon].dropna() - test)
        #print(f"Wilcoxon Test Ä°statistiÄŸi: {test_stat:.4f}")
        print(f"P-DeÄŸeri: {p_value:.2f}")
        if p_value < alpha:
            print(f"SonuÃ§: Medyan {test}'ten anlamlÄ± derecede farklÄ±dÄ±r (H0 reddedildi).")
        else:
            print(f"SonuÃ§: Medyan {test} yakÄ±n olup anlamlÄ± fark yoktur (H0 kabul edildi).")



def guv_aralik(veri,kolon,guven):
    from scipy.stats import t,norm
    import matplotlib.pyplot as plt
    import numpy as np
    ort = veri[kolon].mean()
    std_sap = veri[kolon].std()
    n = veri[kolon].count()  # Ã–rneklem bÃ¼yÃ¼klÃ¼ÄŸÃ¼

    # %95 gÃ¼ven aralÄ±ÄŸÄ± iÃ§in t-daÄŸÄ±lÄ±mÄ±ndan kritik deÄŸeri al
    confidence_level = guven
    alpha = 1 - confidence_level
    degrees_of_freedom = n - 1  # Serbestlik derecesi
    #t_critical = t.ppf(1 - alpha / 2, degrees_of_freedom)  # t-daÄŸÄ±lÄ±mÄ±ndan kritik deÄŸer. gÃ¶zlem deÄŸeri 30'dan kÃ¼Ã§Ã¼k ise
    #z_critical = norm.ppf(1 - alpha / 2)  # z-daÄŸÄ±lÄ±mÄ±ndan kritik deÄŸer. gÃ¶zlem deÄŸeri 30'dan bÃ¼yÃ¼k ise

    ##gÃ¶zlem sayÄ±sÄ±na gÃ¶re kritik deÄŸeri belirle
    if n<30:
        kritik=t.ppf(1 - alpha / 2, degrees_of_freedom)  # t-daÄŸÄ±lÄ±mÄ±ndan kritik deÄŸer. gÃ¶zlem deÄŸeri 30'dan kÃ¼Ã§Ã¼k ise
        uyarÄ±="not: gÃ¶zlem sayÄ±sÄ± 30'dan kÃ¼Ã§Ã¼k olduÄŸu iÃ§in t tablosu kullanÄ±lmÄ±ÅŸtÄ±r."
    else:
        kritik=norm.ppf(1 - alpha / 2)  # z-daÄŸÄ±lÄ±mÄ±ndan kritik deÄŸer. gÃ¶zlem deÄŸeri 30'dan bÃ¼yÃ¼k ise
        uyarÄ±="not: gÃ¶zlem sayÄ±sÄ± 30'dan bÃ¼yÃ¼k olduÄŸu iÃ§in z tablosu kullanÄ±lmÄ±ÅŸtÄ±r."
    # GÃ¼ven aralÄ±ÄŸÄ±nÄ± hesapla
    margin_of_error = kritik * (std_sap / np.sqrt(n))  # Hata payÄ±
    confidence_interval = (ort - margin_of_error, ort + margin_of_error)
    # SonuÃ§larÄ± yazdÄ±r
    
    print(baslik := f"{kolon} deÄŸiÅŸkeni iÃ§in %95 GÃ¼ven AralÄ±ÄŸÄ±:")
    print(ifade2 :=f"  Ortalama: {ort:.2f}")
    print(ifade3 :=f"  GÃ¼ven AralÄ±ÄŸÄ±: ({confidence_interval[0]:.2f}, {confidence_interval[1]:.2f})")
    print(uyarÄ±)
    
    # Grafiksel gÃ¶sterim
    plt.figure(figsize=(8, 4))
    plt.errorbar(x=kolon, y=ort, yerr=margin_of_error, fmt='o', color='blue', 
                 capsize=5, label='Ortalama ve GÃ¼ven AralÄ±ÄŸÄ±')
    plt.title(f"{kolon} iÃ§in %95 GÃ¼ven AralÄ±ÄŸÄ±")
    plt.xlabel("DeÄŸiÅŸken")
    plt.ylabel("DeÄŸer")
    
    
    # Y ekseni sÄ±nÄ±rlarÄ±nÄ± dinamik olarak ayarla
    plt.ylim(ort - 2 * margin_of_error, ort + 2 * margin_of_error)
    
    # Bilimsel gÃ¶sterimi kapat
    plt.ticklabel_format(style='plain', axis='y')
    plt.legend()
    plt.grid(True)
    plt.show()
    sonuc=baslik+"\n"+ifade2+"\n"+ifade3+"\n"+uyarÄ±
    return sonuc





plt.figure(figsize=(10, 10))
plt.pie(fmark["Ori"], labels=fmark["Marka"], autopct='%1.1f%%', startangle=140, wedgeprops={'edgecolor': 'black'})

 BaÅŸlÄ±k ekleme
plt.title("Otomobil MarkalarÄ±nÄ±n DaÄŸÄ±lÄ±mÄ± (%)")

 GrafiÄŸi gÃ¶sterme
plt.show()


plt.figure(figsize=(10, 10))
plt.pie(fmark["Percent"], labels=fmark["Marka"], autopct='%1.1f%%', startangle=140, wedgeprops={'edgecolor': 'black'})

#%%


































df['SÄ±nÄ±f_3'], bins = pd.qcut(df['Sehir_ici'],q=9, retbins=True)
plt.figure(figsize=(10, 5))

plt.xlabel("SÄ±nÄ±f AralÄ±klarÄ±")
plt.ylabel("Frekans")
plt.title("Sabit AralÄ±klÄ± Histogram (cut ile)")
plt.xticks(bins, rotation=45)
plt.show()

sns.histplot(df['Sehir_ici'], bins=9, kde=False, color="skyblue")








df_null_sayisi=df.isnull().sum().to_frame().reset_index().rename(columns={'index':'DeÄŸiÅŸken',0:'sayÄ±'})

df_tanÄ±mlayici_ist=df.describe(include='all').T.reset_index().rename(columns={'index':'DeÄŸiÅŸken'})

df_tanÄ±mlayici_ist.merge(df_null_sayisi,how="left",on="DeÄŸiÅŸken")


df1=df.describe(include='all').T.reset_index()
df1.iloc[5:13,5:11].info()


#deneme=df.groupby('Orijin').describe(include='all').reset_index()
#df_grouped = df.groupby('Orijin').describe(include='all').stack().unstack(level=0).reset_index() # SÃ¼tun isimlerini deÄŸiÅŸtirme df_grouped.columns = ['Orijin', 'Ä°statistik', 'DeÄŸer']

